Durch die Vergabe von Belohnungen und dem übergeordneten Ziel eines Agenten so viele Belohnungen wie möglich zu sammeln, ergibt sich eine spezielle Problematik bei dem Reinforcement Learning, die bei anderen Lernmethoden des Maschinellen Lernens nicht vorhanden ist. Um den Gewinn zu maximieren, muss der Agent auf der einen Seite Aktionen bevorzugen, die sich in der Vergangenheit bereits als gut herausgestellt haben. Er nutzt unvollständige Erfahrung, um so ausbeuterisch wie möglich zu handeln (\textit{Exploitation}). Andererseits ist der Agent dazu gezwungen, neue Aktionen auszuprobieren, damit der Zustands- und Belohnungsraum weiter erkundet wird, um bessere oder sogar optimale Entscheidungen in der Zukunft treffen zu können (\textit{Exploration}). 
\par 
Das Dilemma besteht darin, dass weder Exploration noch Exploitation ausschließlich verfolgt werden kann, ohne dabei die eigentliche Lernaufgabe zum Scheitern zu bringen. Dieses Exploration-Exploitation-Dilemma wird von Mathematikern seit Jahrzehnten intensiv untersucht, bleibt allerdings ungelöst \cite[S.~3]{Sutton1998}. Grundsätzlich muss ein Entscheidungsfinder  eine Reihe von unterschiedlichen Aktionen ausführen und zunehmend jene bevorzugen, die sich als gut herausstellen. Dementsprechend muss eine Balance zwischen den beiden Prozessen gefunden werden.
Eine Strategie, die ausschließlich ausbeuterisch handelt, wird auch als gierig (\textit{greedy}) bezeichnet. Der Begriff \glqq gierig\grqq{} bezeichnet in der Informatik eine Vorgehensweise, bei der immer die, zum Zeitpunkt der Wahl, vermeintlich beste Entscheidung getroffen wird \cite[S.~203]{greedy}. Dabei wird die Suche nach einem globalen Maximum komplett vernachlässigt. Auf den Kontext des Reinforcement Learning übertragen, wählt eine gierige Strategie für jeden Zustand immer jene Aktion, die den derzeitigen größten Nutzen besitzt. Nur wenn die Nutzenfunktion zu einer optimalen Nutzenfunktion konvergiert ist, ist eine gierige Nutzenfunktion auch gleichzeitig die optimale Strategie. Um jedoch die optimale Nutzenfunktion zu finden, muss erkundet werden.
\par
Ein trivialer, aber dennoch effektiver Ansatz ist es, die meiste Zeit gierig zu handeln, aber mit einer geringen Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion auszuführen. Dabei spielen die geschätzten Nutzen der Aktionen keine Rolle und jede Aktion hat die gleiche Wahrscheinlichkeit ausgewählt zu werden. Zu vermerken ist, dass die gierige Aktion $A_*$ ebenfalls in der Menge $\mathcal{A}(S_t)$ enthalten ist. Solche Strategien werden entsprechend als $\epsilon-greedy$ bezeichnet \cite[S.~28]{Sutton1998}:

\begin{equation}\label{eq:greedyProbs}
    \pi(a|S_t) =   
        \begin{cases}
            1-\epsilon + \epsilon / |\mathcal{A}(S_t)|      & \quad \text{wenn } a = A_* \\
            \epsilon / |\mathcal{A}(S_t)|  & \quad \text{wenn } a \neq A_*
        \end{cases}
\end{equation}