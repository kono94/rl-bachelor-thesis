Das Reinforcement Learning wird duch den theoretischen Rahmen der \textit{Markov-Entscheidungsprozesse} beschrieben. Dabei interagiert ein Softwareagent zu diskreten Zeitstempeln mit seiner Umgebung. Als Reaktion auf seine gewählte \textit{Aktion} $a$ zum Zeitpunkt $t$ erhält er einen Zeitpunkt später eine \textit{Belohnung} $r$ und den veränderten \textit{Zustand} $s$ der \textit{Umgebung}.
\par 
Wird die Interaktion durch einen Terminalzustand beendet, so handelt es sich um ein Problem, welches \textit{Episoden} erzeugt. Existiert kein Terminalzustand und somit ein unendlicher Zeithorizont, dann ist dies ein \textit{kontinuierliches} Problem.
\par 
Das Ziel eines jeden RL-Algorithmus ist es, die Summe aller erhaltenen Belohnungen, den sog. \textit{Gewinn}, auf lange Sicht zu maximieren. Für episodale Probleme wird der Gewinn durch die Summe aller erhaltenen Belohnungen pro Episode gebildet. Bei kontinuierlichen Problemen ist dies jedoch nicht möglich, da diese Summe unendlich ist. Aus diesem Grund wird der \textit{Diskontierungsfaktor} $\gamma$ eingeführt. Je kleiner $\gamma$ gewählt ist, desto \glqq kurzsichtiger\grqq{} wird der Agent, da Belohnungen der Zukunft nicht mehr Teil des Gewinns sind.
\par 
Um optimales Verhalten zu erreichen, spielen vor allem zwei Konstrukte eine Rolle. Zum einen der sog. \textit{Nutzen}, der aussagt, wie \glqq gut\grqq{} ein Zustand $q(a)$ oder ein Zustands-Aktions-Paar $q(s,a)$ ist. \glqq Nutzen\grqq{} ist eine Schätzung über die tatsächliche, erwartbare Summe aller Belohnungen, die der Agent erhält, nachdem er sich in einem bestimmten Zustand befindet oder eine bestimmte Aktion in einem Zustand ausgeführt hat. Bei den tabularen Lernmethoden werden alle Werte der Nutzen in einer \glqq Tabelle\grqq{} (oder \textit{Map}) gespeichert, wodurch der Name zustande kommt.  
Zum anderen die \textit{Strategie} $\pi$, die Zustände auf Aktionen mappt und die Entscheidungen bzw. das Verhalten des Agenten steuert. In dieser Arbeit wird hauptsächlich eine $\epsilon$\textit{-greedy} Strategie verwendet, die mit einer Wahrscheinlichkeit von $\epsilon$ eine zufällige Aktion auswählt und mit einer Wahrscheinlichkeit von $1-\epsilon / |\mathcal{A}|$ jene Aktion, die aktuell den höchsten Nutzen besitzt. Diese $\epsilon$\textit{-greedy} Strategie ist zudem ein Lösungsansatz zur Vermeidung des sog. \textit{Exploration-Exploitation-Dilemmas}, da sie sowohl den Zustands- und Aktionsraum erkundet, als auch gierig handelt, um den größten Gewinn zu erhalten.
\par 
Eines der wichtigsten Konzepte, wenn es um Markov-Entscheidungsprozesse und der Modellierung der Zustände für das Reinforcement Learning geht, ist die sog. Markov-Eigenschaft. Diese sagt aus, dass ein Folgezustand nicht abhängig von Aktionen und Zuständen in der Vergangenheit ist, sondern ausschließlich von dem aktuellen Zustand und der aktuell gewählten Aktion. Ein Zustand muss folgerichtig alle nötigen Informationen der Vergangenheit beinhalten, die notwendig sind, um eine optimale Entscheidung treffen zu müssen.