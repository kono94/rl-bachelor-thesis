Fast alle Lernalgorithmen des Reinforcement Learning versuchen eine sog. Nutzenfunktion (\textit{Value Function}) zu schätzen. Diese Funktion sagt aus, \glqq wie gut\grqq{} es ist, dass sich der Agent in einem bestimmten Zustand befindet oder eine bestimmte Aktion in einem Zustand ausführt. Dabei bezieht sich das \glqq wie gut\grqq{} darauf,
welche Belohnungen in der Zukunft erwartbar sind, also wie groß der erwartete Gewinn ist. Zukünftige Belohnungen sind natürlicherweise abhängig davon, wie sich der Agent verhalten bzw. welche Entscheidungen er in der Zukunft treffen wird. Nutzenfunktion sind deshalb immer in Bezug auf eine bestimmte Strategie definiert \cite[S.~58]{Sutton1998}.
\par 
Eine Strategie (\textit{Policy}) kann als Abbildung verstanden werden, die jedem Zustand eine diskrete Wahrscheinlichkeitsverteilung über Aktionen zuordnet. Folgt der Agent einer Strategie $\pi$ zum Zeitpunkt $t$, dann gibt $\pi(a\mid s)$ an, mit welcher Wahrscheinlichkeit $A_t = a$ ausgeführt wird, wenn $S_t = s$ \cite[S.~58]{Sutton1998}. Neben solchen stochastischen Strategien, existieren auch simplere, deterministische Strategien, die jedem Zustand nur genau eine Aktion zuordnen, $\pi (s) = a$ \cite[]{Brunskill}.
\par
Wie anfangs erwähnt, gibt es zwei Varianten der Nutzenfunktion. Die erste sagt aus, wie groß der Erwartungswert des Gewinns für den Zustands $s$ ist, wenn in diesem gestartet und anschließend aufgrund der Strategie $\pi$ gehandelt wird. Dieser \textit{Zustands-Nutzen} kann für alle $s \in \mathcal{S}$ folgendermaßen definiert werden \cite[S.~58]{Sutton1998}:
\begin{equation}\label{eq:valueFunction}
    v_\pi(s) = \EX_\pi[G_t \mid S_t = s] = \EX_\pi[\sum_{k=0}^\infty{\gamma^k R_{t+k+1} \mid S_t = s}]
\end{equation}

Die zweiten Variante gibt Auskunft darüber, wie groß der Nutzen ist, wenn im Zustand $s$ gestartet, daraufhin die Aktion $a$ ausgeführt und anschließend der Strategie $\pi$ gefolgt wird. $q_\pi$ wird auch als \textit{Aktions-Nutzenfunktion} für die Strategie $\pi$ bezeichnet und wird formal ausgedrückt durch \cite[S.~58]{Sutton1998}:
\begin{equation}\label{eq:actionValueFunction}
    q_\pi(s,a) = \EX_\pi[G_t \mid S_t = s, A_t = a] = \EX_\pi[\sum_{k=0}^\infty{\gamma^k R_{t+k+1} \mid S_t = s, A_t = a}]
\end{equation}

//TODO der Erwartungswert auf ?
//TODO funktionsapproximation hier? 