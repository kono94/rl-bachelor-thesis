Fast alle Lernalgorithmen des Reinforcement Learning versuchen eine sog. Nutzenfunktion (\textit{value function}) zu schätzen. Diese Funktion sagt aus, "wie gut" es ist, dass sich der Agent in einem bestimmten Zustand befindet oder eine bestimmte Aktion in einem Zustand auszuführen. Das "wie gut" bezieht sich darauf,
welche Belohnungen in der Zukunft erwartbar sind, also wie der erwartete Gewinn ist. Zukünftige Belohnungen sind natürlicherweise abhängig davon, wie sich der Agent verhalten bzw. welche Entscheidungen er treffen wird. Nutzenfunktion sind deshalb immer in Bezug auf eine bestimmte Strategie definiert\cite[S.~58]{Sutton1998}.
\par 
Eine Strategie (\textit{Policy}) ist die Abbildung von Zuständen auf die Wahrscheinlichkeiten einzelne Aktion auszuführen. Folgt der Agent einer Strategie $\pi$ zum Zeitpunkt $t$, dann gibt $\pi(a\mid s)$ an, mit welcher Wahrscheinlichkeit $A_t = a$ ausgeführt wird, wenn $S_t = s$ \cite[S.~58]{Sutton1998}. Neben solchen stochastischen Strategien, existieren auch simplere, deterministische Strategien, die jedem Zustand nur eine Aktion zuordnen $\pi (s) = a$. 
\par
Wie anfangs erwähnt, gibt es zwei Varianten der Nutzenfunktion. Die erste sagt aus, wie groß der erwartete Gewinn für den Zustands $s$ ist, wenn in diesem gestartet und anschließend aufgrund der Strategie $\pi$ gehandelt wird. Dieser \textit{Zustands-Nutzen} kann für alle $s \in \mathcal{S}$ definiert werden:
\begin{equation}\label{eq:valueFunction}
    v_\pi(s) = \EX_\pi[G_t \mid S_t = s] = \EX_\pi[\sum_{k=0}^\infty{\gamma^k R_{t+k+1} \mid S_t = s}]
\end{equation}

Die zweiten Variante gibt Auskunft darüber wie groß der Nutzen ist, wenn im Zustand $s$ gestartet, daraufhin die Aktion $a$ ausgeführt und anschließend der Stragie $\pi$ gefolgt wird. $q_\pi$ wird auch als Aktion-Nutzen-Funktion für die Stragie $\pi$ bezeichnet und wird formal wie folgt definiert:
\begin{equation}\label{eq:actionValueFunction}
    q_\pi(s,a) = \EX_\pi[G_t \mid S_t = s, A_t = a] = \EX_\pi[\sum_{k=0}^\infty{\gamma^k R_{t+k+1} \mid S_t = s, A_t = a}]
\end{equation}

//TODO der Erwartungswert bezieht sich auf was?
//TODO functionsapproximation hier? 