Die Algorithmen der Dynamischen Programmierung (\textit{Dynamic Programming}, DP) sind im Rahmen dieser Arbeit nicht implementiert und weiter untersucht worden. Dennoch ist ein grundlegendes Verständnis für die DP von Vorteil, da elementare Bestandteile auch in den nachfolgenden Kapiteln zu den Monte-Carlo Methoden und dem Temporal-Difference Learning referenziert werden.
\par 
Die grundlegende Idee der Dynamischen Programmierung ist die Aufteilung eines Optimierungsproblems in Teilprobleme. Dabei wird mit einem trivialem Problem gestartet und die optimale Lösung für jenes in einer Tabelle gespeichert, welches anschließend für die Lösung eines sukzessiv größer werdendes Problem verwendet wird \cite[S.~243]{mehlhorn}.
\par 
Bezogen auf das Reinforcement Learning, ist dieses Problem die Suche nach der optimalen Strategie $\pi_*$ bzw. der Lösung der \textit{Bellman Optimality Equation}, siehe \ref{eq:bellmanValue}. Wie bereits in Kapitel \ref{sec:optimality} erwähnt, ist es möglich dieses Gleichungssystem zu lösen und somit $v_*$ oder $q_*$ linear zu berechnen. Mithilfe der DP erschließt sich jedoch ein iterativer Weg.
\par 
Da die Algorithmen der Dynamischen Programmierung Zugriff auf die Übergangswahrscheinlichkeiten der Umwelt sowie der Belohungsfunktion haben müssen, ist ein perfektes Modell der Umgebung Vorraussetzung. DP-Methoden sind folgerichtig immer \textit{model-based}.
\par

\subsubsection{Strategieevaluierung}
Wichtiger Bestandteil eines jeden RL Algorithmus ist die Berechnung der Zustands-Nutzenfunktion $v_\pi$ (oder Aktions-Nutzenfunktion $q_\pi$) für eine willkürliche Strategie $\pi$. Die geschätzte Nutzenfunktion (unter einer bestimmten Strategie) gegen die wahren Werte der Gewinne streben zu lassen, wird auch als Strategieevaluierung (\textit{Policy Evaluation}) bezeichnet \cite[S.~74]{Sutton1998}.
\par 
Die Vorgehensweise der DP um dieses Vorhersageproblem (\textit{Prediction Problem}) zu lösen, lässt sich folgendermaßen beschreiben. Zunächst wird eine willkürliche Nutzenfunktion $v_0$ gewählt, z.B. sind alle Nutzen zu Beginn mit 0 definiert. Die Bellman-Gleichung wird nun als Aktualisierungsregel angesehen, die Schritt für Schritt die geschätzten Nutzen verbessert. Es entsteht eine Reihenfolge von geschätzten Nutzenfunktionen $v_0$, $v_1$, $v_2$, \dots, die letztendlich zu $v_\pi$ konvergiert. \cite{Sutton1998} definieren die Aktualisierungsregel wie folgt (S.~74):
\begin{equation}\label{eq:fullbackup}
    \begin{aligned}
        \forall s \in \mathcal{S}: v_{k+1}(s) &= \EX_{\pi}[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s] \\
        &= \sum_a{\pi(a|s) \sum_{s',r}p(s',r\mid s, a) \left[r+\gamma v_k(s')\right]}
    \end{aligned}
\end{equation}

Mit Worten beschrieben, wird die Aktualisierungsregel in jeder Iteration auf alle Zustände $s \in \mathcal{S}$ angewendet. Dabei wird der alte Nutzen eines Zustands durch einen neuen Nutzen ersetzt, der auf dem erwarteten Nutzen aller Nachfolgezustände und der sofortigen Belohnung beruht, gewichtet nach den Übergangswahrscheinlichkeiten \cite[S.~20]{Wiering}. Hierbei ist festzustellen, dass die  Aktualisierung des geschätzen Nutzen für einen Zustand auf den ebenfalls geschätzen Nutzen der nachfolgenden Zustände stattfindet. DP-Methoden benutzen also das Prinzip des \textit{bootstrapping} \cite[S.~89]{Sutton1998}.

\subsubsection{Strategieverbesserung}
Ist eine suboptimale Strategie vollständig evaluiert worden, d.h. ist die Nutzenfunktion $v_\pi$ präsent, dann kann diese genutzt werden, um eine besser Strategie zu finden. Dazu wird zunächst $q_\pi$ berechnet durch \cite[S.~21]{Wiering}:

\begin{equation}\label{eq:qBerechnung}
    \begin{aligned}
        q_\pi(s,a) = \EX_\pi\left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=a\right]
    \end{aligned}
\end{equation}

Für den Fall, dass $q_\pi(s,a)$ größer ist als $v_\pi$ für ein $a \in \mathcal{A}$, dann ist es besser die Aktion $a$ auszuführen, als jene Aktion, die durch die aktuelle Strategie $\pi$ gewählt wird. Wird diese Verbesserung (\textit{Policy Improvement}) für alle Zustände durchgeführt, dann ergibt sich die gierige (\textit{greedy}) Strategie $\pi'$, die die besten Aktionen basierend auf die aktuelle Nutzenfunktion ausführt.

\subsubsection{Strategieiteration}
Eine Methode, die die zwei Prozesse zum Evaluieren und der Verbesserung einer Strategie zusammenführt, ist die sog. Strategieiteration \textit{Policy Iteration}, die ihren Ursprung in den Arbeiten von \cite{bellman1957dynamic} und \cite{howard1960dynamic} hat. 
\par 
Bei der Strategieiteration wird zunächst eine willkürliche Strategie $\pi_0$ gewählt, die anschließend zu der Nutzenfunktionen $v_{\pi_k}$ evaluiert wird. Ist dieser Schritt abgeschlossen, wird die Strategie gemäß der berechneten Aktions-Nutzenfunktion $q_{\pi_k}$ (vgl. \ref{eq:qBerechnung}) verbessert, aus $\pi_k$ folgt $\pi_{k+1}$ und eine erneute Evaluation kann erfolgen. Die Schleife wird gestoppt, wenn für alle Zustände $s$ gilt, dass $\pi_{k+1}(s) = \pi_k(s)$ \cite[S.~22]{Wiering}. 
\par 
Die Strategieiteration generiert somit folgende Sequenz \cite[S.~22]{Wiering}:
\begin{equation}\label{eq:policyItSeq}
\pi_0 \rightarrow v_{k_0} \rightarrow  \pi_1 \rightarrow v_{k_1} \rightarrow  \pi_2 \rightarrow v_{k_2}\rightarrow  \pi_3 \rightarrow v_{k_3}\rightarrow \dots \rightarrow \pi_*
\end{equation}

\subsubsection{Nutzeniteration}
In Kapitel 3.1.1 wurde gezeigt, dass die Evaluierung einer Strategie mehrere Iteration durchlaufen muss, um letzendlich zu $v_\pi$ zu konvergieren. Es ist jedoch möglich diesen Prozess vorzeitig abzubrechen, \glqq ohne die Garantie der Konvergenz der Stratgieiteration zu verlieren\grqq{} \cite[S.~82]{Sutton1998}. 
\par 
Diese Erkenntnis macht sich die sog. Nutzeniteration (\textit{Value Iteration}) zu nutzen, die bereits nach einer Iteration der Evaluation abbricht und den Schritt zur Verbesserung der Strategie direkt im Bezug auf diese Berechnung ausführt. Die Nutzeniteration fokusiert sich somit ausschließlich auf Schätzung der Nutzenfunktion und erzeugt im Vergleich zur Strategieiteration folgende Sequenz \cite[S.~23]{Wiering}:

\begin{equation}\label{eq:valueItSeq}
    v_0 \rightarrow v_1 \rightarrow v_2 \rightarrow v_3 \rightarrow \dots \rightarrow v_*
\end{equation}

