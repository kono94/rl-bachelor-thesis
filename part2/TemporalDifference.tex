
Nachdem in den beiden vorigen Kapiteln die Methoden der Dynamischen Programmierung und des Monte-Carlo Ansatzes beleuchtet wurden, befasst sich dieses Kapitel mit der dritten großen Gruppe an Algorithmen, die das Reinforcement Learning Problem lösen, dem \textit{Temporal-Difference Learning} (TD).
Wie zuvor wird zunächst erläutert, wie diese Art der Algorithmen das Vorhersageproblem lösen. Anschließend werden zwei vollständige Algorithmen vorgestellt, die das Kontrollproblem, die Suche nach einer optimalen Strategie, bewältigen. Zugleich werden Unterschiede und Parallelen der drei Lerngruppen aufgezeigt.
\par 
Um einschätzen zu können, welche zentrale Rolle das TD in dem Bereich des Reinforcement Learnings eingenommen hat, folgt ein Zitat von \cite{Sutton1998}:
\begin{quote}
    If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. \cite[S.~119]{Sutton1998}
\end{quote}

Die Verbindung besteht zum einen daraus, dass das TD genau wie die Monte-Carlo Methoden direkt durch die Interaktion mit der Umwelt lernt, folgerichtig auch \textit{model-free} sind. Zum anderen aktualisieren die TD-Methoden, genauso wie bei der Dynamische Programmierung, ihre geschätzten Nutzen mithilfe weiterer geschätzter Nutzen, sie bedienen sich ebenfalls dem Konzept des \textit{bootstrapping} \cite[S.~119]{Sutton1998}. Dadurch ist das TD in der Lage, seine Nutzentabelle nach jeder Aktion zu aktualisieren, \textit{step-by-step}. Ein Warten auf das Ende einer Episode, wie bei den MC-Methoden, ist nicht notwendig. TD kann somit zusätzlich bei kontinuierlichen Problemen zum Einsatz kommen \cite[S.~124]{Sutton1998}.
\par 

\subsubsection{Vorhersageproblem}
MC- und TD-Methoden lösen das Vorhersageproblem beide durch gesammelte Erfahrung durch die direkte Interaktion mit der Umwelt. Sie folgen einer Strategie $\pi$ und aktualisieren ihre geschätzten Nutzen $V$ (oder $Q$) für $v_\pi$ (respektive $q_\pi$) auf Grundlage der erhaltenen Triple $(s,a,r)$. Doch wie schafft es das \textit{Temporal-Differene Learning} im Gegensatz zu den Monte-Carlo Methoden nach jedem Schritt zu aktualisieren und nicht auf das Ende einer Episode zu warten, somit nicht den Gewinn $G_t$ zu benötigen?
\par 
Um diese Frage zu beantworten, wird zunächst ein neuer Parameter vorgestellt, die Schrittgröße $\alpha$ (\textit{step-size parameter}). Dieser Parameter beeinflusst die Lernrate und sorgt konkret dafür, wie stark die Veränderung eines neu geschätzten Nutzens gewichtet wird. Des Weiteren wird der Begriff \glqq Ziel\grqq{} (\textit{target}), im Umfeld von TD auch TD-Ziel (\textit{TD-target}), verwendet. Dieses Ziel sagt aus, zu welchem Wert die derzeitige Aktualisierung des Nutzens streben soll.
\par 
Monte-Carlo Methoden müssen bis zu dem Ende einer Episode warten, da erst dann der Gewinn $G_t$ feststeht, der als Ziel für $V(S_t)$ benötigt wird \cite[S.~119]{Sutton1998}. Eine vereinfachte formale Darstellung der Aktualisierungsregel für die \textit{Every-Visit} MC-Methode sieht wie folgt aus \cite[S.~119]{Sutton1998}:

\begin{equation}
    V(S_t) \gets V(S_t) + \alpha \left[G_t - V(S_t)\right]
\end{equation}

Im Gegensatz dazu müssen die TD-Methoden lediglich bis zu dem nächsten Zeitstempel warten, um eine Aktualisierung vorzunehmen. Dazu wird zum Zeitpunkt $t+1$ sofort ein Ziel gebildet, welches aus der Belohnung $R_{t+1}$ und dem geschätzten Nutzen $V(S_{t+1})$ zusammengesetzt ist. Die Aktualisierungsregel für die einfachste Form des TD lautet somit \cite[S.~120]{Sutton1998}:
\begin{equation}\label{eq:tdupdate}
    V(S_t) \gets V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]
\end{equation}
\par 
Dabei wird der Teil in der eckigen Klammer auch als TD-Fehler (\textit{TD-error}, $\delta$) bezeichnet, der die Differenz zwischen dem geschätzten Wert $S_t$ und dem besseren Schätzwert $R_{t+1} + \gamma V(S_{t+1})$ angibt \cite[S.~121]{Sutton1998}:
\begin{equation}\label{eq:tderror}
    \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{equation}
Statt des Ziels $G_t$ der MC-Methoden, ist das Ziel des TD-Learnings $R_{t+1} + \gamma V(S_{t+1})$. Da der Wert für $V(S_{t+1})$ ein geschätzter Wert ist, aber dennoch für die Aktualisierung verwendet wird, \textit{bootstrappt} das TD-Learning. Dies ist notwendig, um nicht den realen Gewinn nach Abschluss einer Epsiode verwenden zu müssen, sondern diesen gewissermaßen aufspalten zu können und nur auf Basis der aktuellen Belohnung eine Anpassung vorzunehmen. Diese Aufspaltung basiert auf der fundamentalen Erkenntnis, dass Gewinne aufeinanderfolgender Zeitstempel in Verbindung stehen (siehe Kapitel \ref{sec:Gewinne}) und somit gilt \cite[S.~120]{Sutton1998}: 
\begin{equation}\label{eq:targets}
\begin{aligned}
v_\pi &= \EX_\pi\left[G_t \mid S_t = s \right] \\
&= \EX_\pi\left[R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
        &= \EX_\pi\left[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \right]
\end{aligned}
\end{equation}

Anhand von \ref{eq:targets} lässt sich der Zusammenhang der drei großen Gruppen von Lernmethoden sehr gut zusammenfassen. Die erste Zeile beschreibt den geschätzten Wert, den die Monte-Carlo Methoden als Ziel verwenden. Es handelt sich um einen Schätzwert, da der Erwartungswert unbekannt ist und stattdessen mit dem Durchschnitt gesammelter Gewinne gerechnet wird. 
\par 
Die Dynamische Programmierung benutzt den Schätzwert, der sich aus der dritten Zeile ergibt. Dabei bezieht sich das Schätzen nicht auf die eigentlichen Erwartungswerte, denn diese können berechnet werden, weil ein perfektes Modell der Umgebung mit allen Übergangswahrscheinlichkeiten vorhanden ist. Ausschlaggebend ist, dass $v_\pi(S_{t+1})$ zum Zeitpunkt $t$ nicht berechnet, sondern von dem derzeitige geschätzte Nutzen $V_{t+1}$ Gebrauch gemacht wird \cite[S.~120]{Sutton1998}.
\par 
Das TD-Ziel ist eine Schätzung aufgrund beider Gründe, die in den zwei vorigen Absätzen erläutert wurden. Es basiert auf der Sammlung von Erfahrung, um den Erwartungswert bzw. die Werte in der dritten Zeile von \ref{eq:targets} schätzen zu können, und gleichzeitig wird der derzeitig geschätzte Nutzen $V$ anstelle des wahren Wertes von $v_\pi$ verwendet \cite[S.~120f]{Sutton1998}.
\par
\cite{Sutton1998} kommen zu dem Schluss, dass das TD-Learning eine Vereinigung darstellt zwischen der Probenahme (\textit{sampling}) der MC-Methoden und dem \textit{bootstrapping} der DP. Dabei führen die beiden Autoren weiter aus, dass es mit \glqq Vorsicht und Vorstellungskraft\grqq{} möglich sei, die Vorteile beider Methoden (MC und DP) durch den Einsatz des TD-Learnings zu nutzen (S.~120f).
\par
Dass das TD-Verfahren an sich, also die Verwendung der in \ref{eq:tdupdate} dargestellten Aktualisierungsregel, konvergiert, ist durch die Arbeiten von \cite{sutton1988TD} und \cite{dayan} bewiesen worden. Weitere Untersuchungen zu dem Konvergenzverhalten der tabularen TD-Methoden wurden zudem von \cite{jaakkola1994convergence} und \cite{tsitsiklis1994asynchronous} durchgeführt.
\pagebreak
\subsubsection{SARSA}
Nachdem die grundsätzliche Idee des \textit{Temporal-Difference Learnings} im vorigen Unterkapitel erläutert wurde, folgt nun der erste vollständige Algorithmus, um eine optimale Strategie $\pi_*$ zu finden.
\par 
Wie schon bei den Algorithmen der DP und der MC-Methoden, wird das Kontrollproblem auch hier mit dem allgemeinen Leitbild der \textit{Generalized Golicy Iteration}(GPI), siehe Kapitel \ref{sec:GPI}, betrachtet. Das heißt, es werden Aktions-Nutzen geschätzt, $q_\pi(s,a)$, die dann als Grundlage für die Verbesserung der aktuellen ($\epsilon$\textit{-greedy}) Strategie benutzt werden. Entscheidend ist, dass nun eine Form der TD-Aktualisierungsregel (\ref{eq:tdupdate}) für die Strategieevaluierung Verwendung findet.
\par 
In \ref{eq:tdupdate} wurde der Übergang von einem Zustand zu dem nächsten Zustand evaluiert, um den Zustands-Nutzen $V$ zu ermitteln. Zur Berechnung von $Q$ wird bei \textit{SARSA} der Übergang von Zustands-Aktions-Paar zu Zustands-Aktions-Paar betrachtet, einschließlich der Belohnung, die bei diesem Übergang vergeben wird. Es ergibt sich das Quintuple ($S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$), der diesem Algorithmus seinem Namen gibt. Die angepasste Aktualisierungsregel sieht somit wie folgt aus \cite[S.~129]{Sutton1998}:
\begin{equation}
    Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]
\end{equation}
Eingebettet in einen gesamten Algorithmus, ergibt sich nach \cite{Sutton1998} folgender Pseudocode (S.130): 
\pagebreak

\begin{algorithm}
    \caption{Sarsa (on-policy TD control) for estimating $Q \approx q_*$}
    \begin{algorithmic}[1]
        \State Algorithm parameter: step size $\alpha \in (0,1])$, small $\epsilon > 0$
        \State Initialize $Q(s,a),$ for all $s \in S^+, a \in \mathcal{A}(s),$ arbitrarily except that \\ $Q(terminal, \mathord{\cdot}) = 0$
        \\
        \State Loop for each episode:
        \Indent
            \State Initialize $S$
            \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
            \State Loop for each step of episode:
            \Indent
                \State Take action $A$, observe $R, S'$
                \State Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                \State $Q(S,A) \gets Q(S,A) + \alpha [R + \gamma Q(S',A') - Q(S,A)]$
                \State $S \gets S'; A \gets A';$
            \EndIndent
            \State until $S$ is terminal
        \EndIndent 
    \end{algorithmic}
\end{algorithm}
\par 
\textit{SARSA} ist ein sowohl ein \textit{on-policy} als auch ein \textit{online} Algorithmus \cite[S.~129f]{Sutton1998}. \textit{On-policy}, weil die Strategie, die exploriert und die Strategie, die verbessert wird, dieselbe ist. Der Begriff \textit{online} trifft für alle TD-Methoden zu, da sie eine Aktualisierung nach jedem Zeitstempel, also \textit{step-by-step}, durchführen. In einigen Fällen kann dies ein entscheidender Vorteil gegenüber \textit{offline} Methoden sein, wie denen der Monte-Carlo Familie. Wenn bei einer gierigen Strategie eine Aktion den derzeitigen besten Nutzen hat, die dafür sorgt, dass der Agent in einem Zustand festhängt, dann kann die laufende Episode nicht abgeschlossen werden. \textit{Online} Algorithmen wie \textit{SARSA} lernen hingegen während der Episode, dass eine solche Strategie suboptimal ist und wechseln zu einer anderen Strategie \cite[S.~130]{Sutton1998}.
\pagebreak
\subsubsection{Q-Learning}
Eines der frühesten Durchbrüche des Reinforcement Learnings war die Entwicklung des sog. \textit{Q-Learnings}. Dieser Algorithmus wurde von \cite{watkins1989learning} vorgestellt und ist der am weitesten verbreitete RL-Algorithmus. Auf der Seite \textit{arxiv.org}, einem Dokumentenserver für Papers aus dem naturwissenschaftlichen Bereich, liefert die Suche nach den Schlagworten \glqq Reinforcement Learning\grqq{} in Verbindung mit \glqq Q-Learning\grqq{} 774 Treffer, \glqq SARSA\grqq{} hingegen nur 47 und \glqq Monte-Carlo\grqq{} 135. Auch moderne Methoden des \textit{Deep-RLs} basieren auf der Grundlage des \textit{Q-Learnings}, z.B. das \textit{Deep Q-Network} (DQN) \cite{dqn}, welches von \textit{Google DeepMind} entwickelt wurde.
\par 
Der markanteste Unterschied des \textit{Q-Learning} Algorithmus im Vergleich zu \textit{SARSA} ist das Lernen \textit{off-policy}. Im Zusammenhang möglicher Varianten zur fortlaufenden Exploration bei Monte-Carlo Methoden wurde dieses Prinzip im Abschnitt \ref{sec:exploration} bereits erörtert. Es geht dabei um die Eigenschaft, dass zwei unterschiedliche Strategien verwendet werden. Eine interagiert mit der Umwelt und die andere wird schrittweise verbessert, bis sie schließlich zu der optimalen Strategie konvergiert ist. 
\par 
Die erkundende Strategie kann wie bei \textit{SARSA} und den MC-Methoden eine $\epsilon$\textit{-greedy} Strategie sein. 
 Entscheidend ist, dass bei der Aktualisierung der Aktions-Nutzen, $Q$, direkt $q_*$ approximiert wird, indem immer die aktuell beste Aktion des Folgezustands für die Berechnung des geschätzten Gewinns verwendet wird \cite[S.131]{Sutton1998}: 

\begin{equation}
    Q(S,A) \gets Q(S,A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S,A)]
\end{equation}

Die Wahl der besten Aktion in $\gamma \max_a Q(S',a)$ sorgt dafür, dass die Strategie, die die Nutzentabelle aktualisiert, gierig (\textit{greedy}) ist. Da die explorierende Strategie aber $\epsilon$\textit{-greedy} ist, unterscheiden sich diese zwei Strategien voneinander, wodurch sich konkret das \textit{off-policy learning} erklären lässt. Ein kompletter Pseudocode für das \textit{Q-Learning} kann folgendermaßen dargestellt werden \cite[S.~131]{Sutton1998}:
\begin{algorithm}
    \caption{Q-Learning (off-policy TD control) for estimating $\pi \approx \pi_*$}
    \begin{algorithmic}[1]
        \State Algorithm parameter: step size $\alpha \in (0,1])$, small $\epsilon > 0$
        \State Initialize $Q(s,a),$ for all $s \in S^+, a \in \mathcal{A}(s),$ arbitrarily except that \\ $Q(terminal, \mathord{\cdot}) = 0$
        \\
        \State Loop for each episode:
        \Indent
            \State Initialize $S$
            \State Loop for each step of episode:
            \Indent
                \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                \State Take action $A$, observe $R, S'$
                \State $Q(S,A) \gets Q(S,A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S,A)]$
                \State $S \gets S';$
            \EndIndent
            \State until $S$ is terminal
        \EndIndent 
    \end{algorithmic}
\end{algorithm}

\subsubsection{Zusammenfassung}
Das \textit{Temporal-Difference Learning} stellt eine Verbindung zwischen der Dynamischen Programmierung und den Monte-Carlo Methoden dar. Es lernt durch die Interaktion mit der Umwelt, ist also \textit{model-free} wie die MC-Methoden, bedient sich aber zeitgleich dem Konzept des \textit{bootstrapping}, wie es auch bei der DP zum Einsatz kommt. Dadurch ist das TD-Learning in der Lage, ohne ein perfektes Modell nach jeder Aktion zu aktualisieren und braucht nicht auf das Ende einer Episode zu warten.
\par 
Die Suche nach der optimalen Strategie (Kontrollproblem) verläuft wie schon bei den MC-Methoden nach dem Prinzip der \textit{Generalized Policy Iteration}. Konkret wurden zwei Algorithmen zur Bestimmung der optimalen Strategie vorgestellt. Der \textit{SARSA} Algorithmus lernt \textit{on-policy} wohingegen das weit verbreitete \textit{Q-Learning} eine \textit{off-policy} Variante darstellt. Der Unterschied besteht darin, dass bei der \textit{on-policy} Variante eine Strategie für das Sammeln der Erfahrung zuständig ist, die zugleich nach jeder Iteration verbessert wird. Hingegen benutzt das \textit{Q-Learning} zwei Strategien. Eine $\epsilon$-\textit{greedy} Strategie interagiert mit der Umwelt, doch die Aktualiserung der Aktions-Nutzen basiert auf der besten Aktion des Folgezustands, also einer zweiten, rein gierigen Strategie.