\begin{algorithm}
    \caption{Q-Learning (off-policy TD control) for estimating $\pi \approx \pi_*$}
    \begin{algorithmic}[1]
        \State Algorithm parameter: step size $\alpha \in (0,1])$, small $\epsilon > 0$
        \State Initialize $Q(s,a),$ for all $s \in S^+, a \in \mathcal{A}(s),$ arbitrarily except that \\ $Q(terminal, \mathord{\cdot}) = 0$
        \\
        \State Loop for each episode:
        \Indent
            \State Initialize $S$
            \State Loop for each step of episode:
            \Indent
                \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                \State Take action $A$, observe $R, S'$
                \State $Q(S,A) \gets Q(S,A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S,A)]$
                \State $S \gets S';$
            \EndIndent
            \State until $S$ is terminal
        \EndIndent 
    \end{algorithmic}
\end{algorithm}