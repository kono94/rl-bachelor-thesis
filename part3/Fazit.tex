Durch eine ausführliche Auseinandersetzung mit den grundlegenden Bestandteilen des Reinforcement Learnings konnte eine gute, theoretische Basis geschaffen werden, um die Anwendung tabularer RL-Algortihmen auf konkrete Problemstellungen zu bewerten und zu untersuchen. Dabei wurde zunächst der Vorteil der \textit{model-free} Lermethoden erkannt, die im Vergleich zu der Dynamischen Programmierung kein perfektes Modell der Umgebung benötigen.
Zudem wurde festgestellt, dass die Monte-Carlo Methoden ausschließlich  auf Basis von Episoden, wohingegen das \textit{Temporal-Difference Learning} nach jeder Aktion die Aktions-Nutzentabelle anpassen, möglich durch das Prinzip des \textit{bootstrappings}. Die MC-Methoden konnten foglich nur auf \textit{Jumping Dino} Beispiel angewendet werden, die TD-Methoden jedoch zusätzlich auf die kontinuierliche Problemstellung des \textit{AntGames}.
\par 
Vor allem die Untersuchungen im Rahmen des \textit{Jumping Dino} Beispiels offenbarten Interessante Ergebnisse. Zum einen zeigten die gesammelten Daten für das Konvergenzverhalten, dass die TD-Algortihmen \textit{Q-Learning} und \textit{SARSA} in allen Experimenten effizienter waren und schneller zu einer optimalen Strategie konvergierten. Lediglich bei der Belohnungsfunktion, die auf die MC-Methoden zugeschnitten war, schnitt das \textit{Q-Learning} schlechter ab, als die \textit{First-Visit} Variante der MC-Methoden. Allerdings zeigte dies auch, das TD-Methoden flexibler sind, was Zustands- und Belohnungsfunktionmodellierung angeht. Sogar die fehlende Informationen, ob sich der Dino derzeit im Sprung befindet oder nicht, hinderten die TD-Methoden nicht an dem Erreichen einer optimalen Strategie. 
\par 
Die fehlende \glqq inJump\grqq{} Information enthüllte zugleich die Unterschiede zwischen der \textit{First-Visit} und der \textit{Every-Visit} Variante der Monte-Carlo Methoden. Es konnte bewiesen werden, dass für das konkrete Beispiel des \textit{Jumping Dino Simple} die Herangehensweise der \textit{First-Visit} ungeeignet ist und nicht zu einer Konvergenz führen kann, wohingegen die \textit{Every-Visit} Variante sehr wohl in der Lage ist, diese zu erreichen.
\par 
Warum allerdings die TD-Methoden nicht bei dem \textit{Jumping Dino Advanced} konvergierten, konnte nicht begründet werden und Bedarf weiterer Untersuchungen.
\par 
Durch zahlreiche Experimenten, die unterschiedliche Werte für die jeweiligen Lernparameter untersuchten, konnten weitere Erkenntnisse gewonen werden. So wirken sich zu große Werte (> 0.8) für den Explorationsfaktor $\epsilon$ negativ auf das Konvergenzverhalten aus, weil zu willkürlich gehandelt wird lange Entscheidungsketten frühzeitig durch suboptimale Entscheidungen unterbrochen werden. Auch zu kleine Werte (<= 0.1) haben einen negativen Einfluss. Die Ergebnisse legen offen, dass ein für $\epsilon$ um 0.5 am besten geeignet ist, der schrittweise durch aufgabenespezifische Schranken verringert wird und letztendlich 0 erreicht.
\par 
Daneben konnten die Analysen im Rahmen des \textit{AntGames} zeigen, dass bei stationären Problemen, also Probleme, deren Umwelt und Zustand- und Aktionsraum stets unverändert bleibt, eine hohe Lernrate $\alpha$ zu präferieren ist.
\par 
Eines der wichtigsten Erkenntnisse dieser Arbeit ist, dass der Diskontierungsfaktor $\gamma$ einen entscheidenen Einfluss auf den Erfolg der Lermethoden bei kontinuierlichen Problemen hat. Bei episodialen Problemen wird stets ein Wert von 1 gewählt, doch ist dies bei kontinuierlichen Entscheidungsproblemen in der Theorie nicht möglich, die Gründe dafür wurden in Kapitel \ref{Gewinne} erläutert. Ist ein zu geringer Wert für $\gamma$ gewählt worden, dann ist der Agent nicht fähig optimales Verhalten zu erlernen, da er zu \glqq kurzsichtig\grqq{} wird und keine langen Entscheidungssequenzen aufbauen kann. Um dieses Problem zu umgehen kann immer ein Wert sehr nahe an 1 vorgezogen werden, beispielsweise 0.99. Jedoch führt dies zu deutlich höherem Lernaufwand, der in dem konrekten Beispiel des \textit{AntGames} den Faktor 15 betrug.
\par
Ein tiefes Verständnis über die Vorgehensweise der Algorithmen und der unterschiedlichen Lernparameter, sowie eine detaillierte Auseinandersetzung mit der gestellten Problemszenario muss somit gegeben sein, um RL-Verfahren erfolgreich anwenden zu können.
\par 
Ist dies jedoch der Fall, dann konnte bewiesen werden, dass das \textit{Q-Learning} tatsächlich zu einem Verhalten für das \textit{AntGame} konvergiert, welches identisch zu dem eines Wegfindungsalgorithmus, wie dem \textit{A-Star}, ist. Die ursprüngliche Aufgabenstellung wurde allerdings merklich umformuliert genauer gesagt wurde die Zustandsmodellierung deutlich anders gestaltet, um die Markov-Eigenschaft zu bedienen. Das Prinzip des Gestanks und Geruchs bei Futter respektive Fallen wurde komplett verworfen und der Zustand besteht aus  den Informationen über die gesamte Rasterwelt. Dennoch ist eine solche Zustandsmodellierung, die gewissermaßen einen gesamten Frame der Umwelt abbildet sehr verbreitet in der Anwendung von RL-Algorithmen in der Spieltheorie. Das bekannteste Beispiel für diese Art der Modellierung ist u.a. das Erlernen sämtlicher \textit{Atari}-Spiele auf menschlichem Niveau \cite{dqn}.
\par 
