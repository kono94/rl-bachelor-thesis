Durch eine ausführliche Auseinandersetzung mit den grundlegenden Bestandteilen des Reinforcement Learnings konnte eine theoretische Basis geschaffen werden, um die Anwendung tabularer RL-Algorithmen auf konkrete Problemstellungen zu bewerten und zu untersuchen. Dabei wurde zunächst der Vorteil der \textit{model-free} Lernmethoden erkannt, die im Vergleich zu der Dynamischen Programmierung kein perfektes Modell der Umgebung benötigen.
Zudem wurde festgestellt, dass die Monte-Carlo Methoden ausschließlich  auf Basis von Episoden lernt, wohingegen das \textit{Temporal-Difference Learning} nach jeder Aktion die Aktions-Nutzen-Tabelle anpasst, möglich durch das Prinzip des \textit{bootstrappings}. Die MC-Methoden konnten folglich nur auf \textit{Jumping Dino} Beispiel angewendet werden, die TD-Methoden jedoch zusätzlich auf die kontinuierliche Problemstellung des \textit{AntGames}.
\par 
Vor allem die Untersuchungen im Rahmen des \textit{Jumping Dino} Beispiels offenbarten interessante Ergebnisse. Zum einen zeigten die gesammelten Daten für das Konvergenzverhalten, dass die TD-Algorithmen \textit{Q-Learning} und \textit{SARSA} in allen Experimenten effizienter waren und schneller zu einer optimalen Strategie konvergierten. Lediglich bei der Belohnungsfunktion, die auf die MC-Methoden zugeschnitten war, schnitt das \textit{Q-Learning} schlechter ab als die \textit{First-Visit} Variante der MC-Methoden. Allerdings zeigte dies auch, dass TD-Methoden flexibler sind, was Zustands- und Belohnungsfunktionsmodellierung angeht. Sogar die fehlende Information, ob sich der Dino derzeit im Sprung befindet oder nicht (\glqq inJump\grqq{}), hinderten die TD-Methoden nicht an dem Erreichen einer optimalen Strategie. 
\par 
Die fehlende \glqq inJump\grqq{} Information enthüllte zugleich die Unterschiede zwischen der \textit{First-Visit} und der \textit{Every-Visit} Variante der Monte-Carlo Methoden. Es konnte bewiesen werden, dass für das konkrete Beispiel des \textit{Jumping Dino Simple} die Herangehensweise der \textit{First-Visit} ungeeignet ist und nicht zu einer Konvergenz führen kann, wohingegen die \textit{Every-Visit} Variante sehr wohl in der Lage ist, diese zu erreichen.
\par 
Warum allerdings die TD-Methoden nicht bei dem \textit{Jumping Dino Advanced} konvergierten, konnte nicht begründet werden und bedarf weiterer Untersuchungen.
\par 
Durch zahlreichen Experimente, die unterschiedliche Werte für die jeweiligen Lernparameter untersuchten, konnten weitere Erkenntnisse gewonnen werden. So wirken sich zu große Werte (> 0.8) für den Explorationsfaktor $\epsilon$ negativ auf das Konvergenzverhalten aus, weil zu willkürlich gehandelt wird und lange Entscheidungsketten frühzeitig durch suboptimale Entscheidungen unterbrochen werden. Auch zu kleine Werte (<= 0.1) haben einen negativen Einfluss. Die Ergebnisse legen offen, dass ein Wert für $\epsilon$ um 0.5 am besten geeignet ist, der schrittweise durch aufgabenspezifische Schranken verringert wird und letztendlich 0 erreicht.
\par 
Daneben konnten die Analysen im Rahmen des \textit{AntGames} zeigen, dass bei stationären Problemen, also Probleme, deren Umwelt und Zustands- und Aktionsraum stets unverändert bleibt, eine hohe Lernrate $\alpha$ zu präferieren ist.
\par 
Eines der wichtigsten Erkenntnisse dieser Arbeit ist, dass der Diskontierungsfaktor $\gamma$ einen entscheidenen Einfluss auf den Erfolg der Lernmethoden bei kontinuierlichen Problemen hat. Bei episodialen Problemen wird stets ein Wert von 1 gewählt, doch ist dies bei kontinuierlichen Entscheidungsproblemen in der Theorie nicht möglich, die Gründe dafür wurden in Kapitel \ref{sec:Gewinne} erläutert. Ist ein zu geringer Wert für $\gamma$ gewählt worden, dann ist der Agent nicht fähig optimales Verhalten zu erlernen, da er zu \glqq kurzsichtig\grqq{} wird und keine langen Entscheidungssequenzen aufbauen kann. Um dieses Problem zu umgehen kann immer ein Wert sehr nahe an 1 vorgezogen werden, beispielsweise 0.99. Jedoch führt dies zu deutlich höherem Lernaufwand, der in dem Beispiel des \textit{AntGames} den Faktor 15 betrug.
\par
Ein tiefes Verständnis über die Vorgehensweise der Algorithmen und der unterschiedlichen Lernparameter, sowie eine detaillierte Auseinandersetzung mit dem gestellten Problemszenario muss somit gegeben sein, um RL-Verfahren erfolgreich anwenden zu können.
\par 
Ist dies jedoch der Fall, dann konnte bewiesen werden, dass das \textit{Q-Learning} tatsächlich zu einem Verhalten für das \textit{AntGame} konvergiert, welches identisch zu dem eines Wegfindungsalgorithmus, wie dem \textit{A-Star}, ist. Die ursprüngliche Aufgabenstellung wurde allerdings merklich umformuliert, genauer gesagt wurde die Zustandsmodellierung deutlich anders gestaltet, um die Markov-Eigenschaft zu bedienen. Das Prinzip des Gestanks und Geruchs bei Futter respektive Fallen wurde komplett verworfen und der Zustand besteht aus  den Informationen über die gesamte Rasterwelt. Dennoch ist eine solche Zustandsmodellierung, die gewissermaßen einen gesamten Rahmen der Umwelt abbildet, sehr verbreitet in der Anwendung von RL-Algorithmen in der Spieltheorie. Das bekannteste Beispiel für diese Art der Modellierung ist u.a. das Erlernen sämtlicher \textit{Atari}-Spiele auf menschlichem Niveau \cite{dqn}.
\par 
