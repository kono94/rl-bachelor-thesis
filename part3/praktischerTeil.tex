Nachdem die theoretischen Grundlagen in den vorigen Kapitel ausführlich beleuchtet worden sind, folgt nun eine praktische Auseinandersetzung durch die Anwendung verschiedener Lernmethoden des Reinforcement Learnings auf zwei konkrete Problemszenarien. 
\par 
Dabei wird der Leser zunächst mit der, von Grund auf erstellten, Implementierung vertraut gemacht. Anforderungen und Vorraussetzungen werden kurz erläutert und der Grundaufbau wird anhand der wichtigsten Interfaces dargestellt. Anschließend wird auf die beiden Problemstellungen eingegangen. 
\par 
Neben einer detaillierten Beschreibung der Umwelt, wird der Fokus auf die Zustands- und Belohnungsfunktionsmodellierung gelegt. Geanuer geht es um die Fragestellung, wie eine Modellierung konkret auszusehen hat, damit Rl-Algorithmen auf das Problem anwendbar sind und letztendlich zu einem positiven Ergebnis, dem optimalen Verhalten, streben.
\par
Die Bezeichnungen für die unterschiedliche Varianten der Problemstellungen sowie die zahlreichenden Zustands- und Belohnungsfunktionsmodellierung, die in diesem Kapitel verwendet werden, gelten als Grundlage für die Ergebnisse, die im nächsten Kapitel vorgestellt werden. Als Beispiel eine Analyse des Jumping Dino Problems: \{$FirstVisitMonteCarlo,Simple, Z_2, B_1$\}. Damit ist gemeint, dass die \textit{First-Visit }Variante der Monte-Carlo Methoden als Lernalgorithmus auf das Problemszenarien Jumping Dino \textit{Simple} unter Verwendung der Zustandsmodellierung $Z_2$ und der Belohnungsfunktion $B_1$, die jeweils in den Unterabschnitten \glqq Zustandsmodellierung\grqq{} respektive \glqq Belohnungsfunktion\grqq{} erläutert sind, angewendet wurde.
